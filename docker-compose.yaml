services:
  spark-master:
    image: apache/spark:latest
    container_name: spark-master
    restart: unless-stopped
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8080:8080" # Spark Master Web UI
      - "${SPARK_MASTER_PORT}:${SPARK_MASTER_PORT}" # Spark Master RPC
    volumes:
      - ./data:/opt/spark-data
      - ./output:/opt/spark-output
    networks:
      - spark-network
    
  spark-worker-1:
    image: apache/spark:latest
    container_name: spark-worker-1
    restart: unless-stopped
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker ${SPARK_MASTER_URL}
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081" # Worker Web UI
    volumes:
      - ./data:/opt/spark-data
      - ./output:/opt/spark-output
    depends_on:
      - spark-master
    networks:
      - spark-network
  
  spark-worker-2:
    image: apache/spark:latest
    container_name: spark-worker-2
    restart: unless-stopped
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker ${SPARK_MASTER_URL}
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8082:8081"  # Worker Web UI
    volumes:
      - ./data:/opt/spark-data
      - ./output:/opt/spark-output
    depends_on:
      - spark-master
    networks:
      - spark-network
  
  spark-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-app
    restart: unless-stopped
    environment:
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - ./data:/opt/spark-data
      - ./output:/opt/spark-output
      - ./src:/opt/spark-app
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    networks:
      - spark-network
    command: tail -f /dev/null

networks:
  spark-network:
    driver: bridge
